{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GalaxyKickStart \u00b6 GalaxyKickStart is an Ansible playbook designed for installing, testing, deploying and maintaining production-grade Galaxy server instances. GalaxyKickStart playbook code is available in GitHub . In the basic configuration, the deployed Galaxy servers include: a postgresql server as database backend a uwsgi Web Server Gateway Interface a Slurm job manager a nginx web proxy a proftpd FTP server the service manager supervisor","title":"Home"},{"location":"#galaxykickstart","text":"GalaxyKickStart is an Ansible playbook designed for installing, testing, deploying and maintaining production-grade Galaxy server instances. GalaxyKickStart playbook code is available in GitHub . In the basic configuration, the deployed Galaxy servers include: a postgresql server as database backend a uwsgi Web Server Gateway Interface a Slurm job manager a nginx web proxy a proftpd FTP server the service manager supervisor","title":"GalaxyKickStart"},{"location":"GKS2slurm/","text":"Unstable development \u00b6 GKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation. The playbook GKS2slurm galaxyToSlurmCluster.yml was tested with multiple virtual machines (VMs) in Stratuslab , Google Cloud Engine (GCE) and Amazon Web Services (AWS) clouds. Installation of a Galaxy slurm cluster with GKS2slurm \u00b6 Step 1: Install a Galaxy server with GalaxyKickStart \u00b6 Report to the Getting Started section of this manual for the basics of GalaxyKickStart installation install any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice. Flavors currently available are kickstart , artimed and metavisitor but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows. in Step 1, the most important thing to keep track with is to configure your target machine with an extra volume Indeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up. Thus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the group_vars/all file and adapt the galaxy_persistent_directory variable to your extra volume which should be already formatted and mounted: Change #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export To #persistent data galaxy_persistent_directory: /pathto/mounted/extravolume Having configured your GalaxyKickStart installation, import the extra roles (if not already done) ansible-galaxy install -r requirements_roles.yml -p roles and run the galaxy.yml playbook: ansible-playbook --inventory-file inventory_files/<your_inventory_file> galaxy.yml Step 2: Check the single node Galaxy installation \u00b6 If the playbook was run successfully, connect to your Galaxy instance through http and check that you can login ( admin@galaxy.org :admin), and that tools and workflows are correctly installed. Step 3: Moving your single node configuration to a multinode slurm configuration \u00b6 Start as many compute nodes you want for the slurm cluster and gather information from each node: IP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes) hostname number of CPUs memory (in MB) Step 3-1 \u00b6 Adapt the inventory file slurm-kickstart in the inventory_files folder. [slurm_master] # adapt the following line to IP address and ssh user of the slurm master node 192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" [slurm_slave] # adapt the following lnes to IP addresses and ssu users of the slum slave nodes 192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" 192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" 192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" Step 3-2 \u00b6 Adapt the group_vars file slurm_master in the group_vars folder. This is done using the information gathered in step 3 # nfs sharing cluster_ip_range: \"0.0.0.0/24\" # replace by your ip network range # slave node specifications, adapt to your set of slave nodes slave_node_dict: - {hostname: \"slave-1\", CPUs: \"2\", RealMemory: \"7985\"} - {hostname: \"slave-2\", CPUs: \"2\", RealMemory: \"7985\"} - {hostname: \"slave-3\", CPUs: \"2\", RealMemory: \"7985\"} Step 3-3 \u00b6 Adapt the group_vars file slurm_slave in the group_vars folder # adapt the following variable to the master slurm node IP address master_slurm_node_ip: \"192.54.201.102\" Step 3-4 \u00b6 Run the playbook galaxyToSlurmCluster.yml playbook. from the GalaxyKickStart directory: ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Note that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False: ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Checking slurm installation \u00b6 Connect to your master node as root and type sinfo Refer to slurm documentation for more investigation/control","title":"GKS2slurm"},{"location":"GKS2slurm/#unstable-development","text":"GKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation. The playbook GKS2slurm galaxyToSlurmCluster.yml was tested with multiple virtual machines (VMs) in Stratuslab , Google Cloud Engine (GCE) and Amazon Web Services (AWS) clouds.","title":"Unstable development"},{"location":"GKS2slurm/#installation-of-a-galaxy-slurm-cluster-with-gks2slurm","text":"","title":"Installation of a Galaxy slurm cluster with GKS2slurm"},{"location":"GKS2slurm/#step-1-install-a-galaxy-server-with-galaxykickstart","text":"Report to the Getting Started section of this manual for the basics of GalaxyKickStart installation install any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice. Flavors currently available are kickstart , artimed and metavisitor but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows. in Step 1, the most important thing to keep track with is to configure your target machine with an extra volume Indeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up. Thus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the group_vars/all file and adapt the galaxy_persistent_directory variable to your extra volume which should be already formatted and mounted: Change #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export To #persistent data galaxy_persistent_directory: /pathto/mounted/extravolume Having configured your GalaxyKickStart installation, import the extra roles (if not already done) ansible-galaxy install -r requirements_roles.yml -p roles and run the galaxy.yml playbook: ansible-playbook --inventory-file inventory_files/<your_inventory_file> galaxy.yml","title":"Step 1: Install a Galaxy server with GalaxyKickStart"},{"location":"GKS2slurm/#step-2-check-the-single-node-galaxy-installation","text":"If the playbook was run successfully, connect to your Galaxy instance through http and check that you can login ( admin@galaxy.org :admin), and that tools and workflows are correctly installed.","title":"Step 2: Check the single node Galaxy installation"},{"location":"GKS2slurm/#step-3-moving-your-single-node-configuration-to-a-multinode-slurm-configuration","text":"Start as many compute nodes you want for the slurm cluster and gather information from each node: IP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes) hostname number of CPUs memory (in MB)","title":"Step 3: Moving your single node configuration to a multinode slurm configuration"},{"location":"GKS2slurm/#step-3-1","text":"Adapt the inventory file slurm-kickstart in the inventory_files folder. [slurm_master] # adapt the following line to IP address and ssh user of the slurm master node 192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" [slurm_slave] # adapt the following lnes to IP addresses and ssu users of the slum slave nodes 192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" 192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\" 192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file=\"~/.ssh/mysshkey\"","title":"Step 3-1"},{"location":"GKS2slurm/#step-3-2","text":"Adapt the group_vars file slurm_master in the group_vars folder. This is done using the information gathered in step 3 # nfs sharing cluster_ip_range: \"0.0.0.0/24\" # replace by your ip network range # slave node specifications, adapt to your set of slave nodes slave_node_dict: - {hostname: \"slave-1\", CPUs: \"2\", RealMemory: \"7985\"} - {hostname: \"slave-2\", CPUs: \"2\", RealMemory: \"7985\"} - {hostname: \"slave-3\", CPUs: \"2\", RealMemory: \"7985\"}","title":"Step 3-2"},{"location":"GKS2slurm/#step-3-3","text":"Adapt the group_vars file slurm_slave in the group_vars folder # adapt the following variable to the master slurm node IP address master_slurm_node_ip: \"192.54.201.102\"","title":"Step 3-3"},{"location":"GKS2slurm/#step-3-4","text":"Run the playbook galaxyToSlurmCluster.yml playbook. from the GalaxyKickStart directory: ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Note that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False: ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml","title":"Step 3-4"},{"location":"GKS2slurm/#checking-slurm-installation","text":"Connect to your master node as root and type sinfo Refer to slurm documentation for more investigation/control","title":"Checking slurm installation"},{"location":"GKSfromWorflows/","text":"Unstable development \u00b6 GKSfromWorflows uses the python script scripts/galaxykickstart_from_workflow.py to quickly generate a GalaxyKickStart use case from one or several workflow files ( .ga , note that these files must have been generated with galaxy >= release_16.04) From GalaxyKickStart/scripts , run \u00b6 python galaxykickstart_from_workflow.py --help Then python galaxykickstart_from_workflow.py -w <workflow1.ga> <workflow2.ga> ... -l <Panel_label> This creates: An inventory file GKSfromWorkflow in the inventory_files folder A group_vars file GKSfromWorkflow in the group_vars folder A folder GKSfromWorkflow in the folder extra-files which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a GKSfromWorkflow_tool_list.yml file that contains a yml description of all tools used in the workflows. Note that running galaxykickstart_from_workflow.py overwrites these folders and files if they exist from a previous script run. Adapt the created inventory file \u00b6 Before running ansible-playbook, you have just to adapt the GKSfromWorkflow inventory_file inventory_files/GKSfromWorkflow to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine). As usual, you may also tune the group_vars/all file. Run the playbook \u00b6 cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles ansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml Check your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....","title":"GKSfromWorflows"},{"location":"GKSfromWorflows/#unstable-development","text":"GKSfromWorflows uses the python script scripts/galaxykickstart_from_workflow.py to quickly generate a GalaxyKickStart use case from one or several workflow files ( .ga , note that these files must have been generated with galaxy >= release_16.04)","title":"Unstable development"},{"location":"GKSfromWorflows/#from-galaxykickstartscripts-run","text":"python galaxykickstart_from_workflow.py --help Then python galaxykickstart_from_workflow.py -w <workflow1.ga> <workflow2.ga> ... -l <Panel_label> This creates: An inventory file GKSfromWorkflow in the inventory_files folder A group_vars file GKSfromWorkflow in the group_vars folder A folder GKSfromWorkflow in the folder extra-files which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a GKSfromWorkflow_tool_list.yml file that contains a yml description of all tools used in the workflows. Note that running galaxykickstart_from_workflow.py overwrites these folders and files if they exist from a previous script run.","title":"From GalaxyKickStart/scripts, run"},{"location":"GKSfromWorflows/#adapt-the-created-inventory-file","text":"Before running ansible-playbook, you have just to adapt the GKSfromWorkflow inventory_file inventory_files/GKSfromWorkflow to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine). As usual, you may also tune the group_vars/all file.","title":"Adapt the created inventory file"},{"location":"GKSfromWorflows/#run-the-playbook","text":"cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles ansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml Check your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....","title":"Run the playbook"},{"location":"about/","text":"GalaxyKickStart \u00b6 GalaxyKickStart is an Ansible playbook designed to deploy one or more production-ready Galaxy servers based on Ubuntu within ~30 minutes, and to maintain these servers. GalaxyKickStart can also install tools and workflows in the deployed Galaxy servers. Requirements \u00b6 The Galaxykickstart playbook is tested in Ubuntu 16.04, 18.04 and 20.04 , with ansible >= 9.2.6 , and a target machine pre-installed with Python 3.7 . Target environments \u00b6 The GalaxyKickStart playbook is primarily tested (ci) in virtual machines using GitHub Actions workflows. ARTbio is also using the playbook to install and maintain its bare metal Galaxy servers or its virtual servers in Google Cloud Platform . Finally, GalaxyKickStart can be used to build your production-ready Docker image and a Galaxy Docker image is freely available using docker pull artbio/galaxykickstart:18.04 . The GalaxyKickStart Ansible playbook is maintained by ARTbio platform and makes use of roles which have been developed by the Galaxy team . To ensure the GalaxyKickStart stability, these roles (listed below) are forked and maintained separately in ARTbio GitHub repositories (in the galaxykickstart branches). List of dependency roles included in this playbook: \u00b6 galaxy-os role natefoo-postgresql_objects ensure_postgresql_up galaxy role miniconda-role galaxy-extras role galaxy-trackster role galaxy-tools role","title":"What is GalaxyKickStart"},{"location":"about/#galaxykickstart","text":"GalaxyKickStart is an Ansible playbook designed to deploy one or more production-ready Galaxy servers based on Ubuntu within ~30 minutes, and to maintain these servers. GalaxyKickStart can also install tools and workflows in the deployed Galaxy servers.","title":"GalaxyKickStart"},{"location":"about/#requirements","text":"The Galaxykickstart playbook is tested in Ubuntu 16.04, 18.04 and 20.04 , with ansible >= 9.2.6 , and a target machine pre-installed with Python 3.7 .","title":"Requirements"},{"location":"about/#target-environments","text":"The GalaxyKickStart playbook is primarily tested (ci) in virtual machines using GitHub Actions workflows. ARTbio is also using the playbook to install and maintain its bare metal Galaxy servers or its virtual servers in Google Cloud Platform . Finally, GalaxyKickStart can be used to build your production-ready Docker image and a Galaxy Docker image is freely available using docker pull artbio/galaxykickstart:18.04 . The GalaxyKickStart Ansible playbook is maintained by ARTbio platform and makes use of roles which have been developed by the Galaxy team . To ensure the GalaxyKickStart stability, these roles (listed below) are forked and maintained separately in ARTbio GitHub repositories (in the galaxykickstart branches).","title":"Target environments"},{"location":"about/#list-of-dependency-roles-included-in-this-playbook","text":"galaxy-os role natefoo-postgresql_objects ensure_postgresql_up galaxy role miniconda-role galaxy-extras role galaxy-trackster role galaxy-tools role","title":"List of dependency roles included in this playbook:"},{"location":"available_roles/","text":"ansible-postgresql-objects role ensure_postgresql_up role galaxy-os role miniconda role galaxy role galaxy-extras role ansible-trackster role galaxy-tools role","title":"Available roles"},{"location":"available_variables/","text":"Most of the available ansible variables used by the galaxykickstart playbook scripts galaxy.yml and galaxy_tool_install.yml can be found in the group_vars/all file and are reproduced below, with there default values: GKS: true galaxy_virtualenv_python: python3 ansible_python_interpreter: python3 proxy_env: {} install_galaxy: true install_maintainance_packages: false galaxy_manage_trackster: true galaxy_create_user: true # Privilege separation mode switch galaxy_separate_privileges: true # User that Galaxy runs as galaxy_user: \"{{ galaxy_user_name }}\" # User that owns Galaxy code, configs, and virutalenv, and that runs `pip install` for dependencies galaxy_privsep_user: \"{{ galaxy_user_name }}\" gks_run_data_managers: false galaxy_hostname: \"{{ inventory_hostname }}\" nginx_galaxy_location: \"\" # galaxy_become_users: {} # for compatibility with ansible.galaxy. as doc seems to say pip_virtualenv_command: virtualenv galaxy_user_name: galaxy galaxy_user_gid: 1450 galaxy_user_uid: 1450 postgres_user_uid: 1550 postgres_user_gid: 1550 galaxy_server_dir: /home/{{ galaxy_user_name }}/{{ galaxy_user_name }} galaxy_venv_dir: \"{{ galaxy_server_dir }}/.venv\" galaxy_local_tools_dir: \"{{ galaxy_server_dir }}/tools\" galaxy_data: /home/{{ galaxy_user_name }}/{{ galaxy_user_name }} galaxy_config_dir: \"{{ galaxy_server_dir }}/config\" galaxy_database: /home/galaxy_database galaxy_db: postgresql://{{ galaxy_user_name }}:{{ galaxy_user_name }}@localhost:5432/galaxy?client_encoding=utf8 galaxy_git_repo: https://github.com/galaxyproject/galaxy.git galaxy_changeset_id: release_20.05 galaxy_reports_config_file: \"{{ galaxy_config_dir }}/reports.yml.sample\" # Change this to \"{{ galaxy_config_dir }}/reports.ini.sample\" for galaxy < 17.09 galaxy_admin: admin@galaxy.org galaxy_admin_pw: artbio2020 # use the most recent PBKDF2 function for galaxy (and proftpd) authentication use_pbkdf2: true proftpd_sql_auth_type: PBKDF2 proftpd_files_dir: \"{{ galaxy_data }}/database/ftp\" postgresql_version: \"{{ '9.3' if ansible_distribution=='Ubuntu' and ansible_distribution_version is version('15.04', '<=') else '9.5' if ansible_distribution=='Ubuntu' and ansible_distribution_version is version('17.04', '<') else '10' if ansible_distribution=='Ubuntu' and ansible_distribution_version is version('18.04', '<=') else '12' if ansible_distribution=='Ubuntu' and ansible_distribution_version is version('20.04', '<=') else '9.4' if ansible_distribution=='Debian' and ansible_distribution_version is version('8.0', '>=') }}\" supervisor_postgres_database_path: /var/lib/postgresql/{{ postgresql_version }}/main supervisor_postgres_database_config: /etc/postgresql/{{ postgresql_version }}/main/postgresql.conf supervisor_postgres_options: \"-D {{ supervisor_postgres_database_path }} -c \\\"config_file={{ supervisor_postgres_database_config }}\\\"\" # set to 'no' to skip reinitialisation and clear docker images (docker must then be already installed) configure_docker: yes galaxy_extras_apt_package_state: present install_apparmor: false tool_dependency_dir: /home/{{ galaxy_user_name }}/tool_dependencies shed_tools_dir: \"{{ galaxy_server_dir }}/../shed_tools\" tool_data_dir: \"{{ galaxy_server_dir }}/tool-data\" galaxy_mutable_data_dir: \"{{ galaxy_server_dir }}/database\" miniconda_python: 3 miniconda_version: \"4.6.14\" miniconda_installer_checksum: \"\" miniconda_prefix: \"{{ tool_dependency_dir }}/_conda\" miniconda_manage_dependencies: False additional_files_list: - { src: \"extra-files/tool_sheds_conf.xml\", dest: \"{{ galaxy_config_dir }}\" } supervisor_env_vars: # Use pre-exsting env vars if they are defined - export IP_ADDRESS=${IP_ADDRESS:-`curl --silent icanhazip.com`} - export MASQUERADE_ADDRESS=${MASQUERADE_ADDRESS:-$IP_ADDRESS} - export GALAXY_GID=\"${GALAXY_GID:-{{ galaxy_user_gid }}}\" - export GALAXY_UID=\"${GALAXY_UID:-{{ galaxy_user_uid }}}\" - export NATIVE_SPEC=\"${NATIVE_SPEC:---ntasks=`/usr/bin/nproc` --share}\" - export NGINX_GALAXY_LOCATION=\"${NGINX_GALAXY_LOCATION:-{{ nginx_galaxy_location }}}\" - export GALAXY_CONFIG_FTP_UPLOAD_SITE=\"${GALAXY_CONFIG_FTP_UPLOAD_SITE:-ftp://$IP_ADDRESS}\" - export GALAXY_CONFIG_NGINX_X_ACCEL_REDIRECT_BASE=\"${GALAXY_CONFIG_NGINX_X_ACCEL_REDIRECT_BASE:-$NGINX_GALAXY_LOCATION/_x_accel_redirect}\" - export GALAXY_CONFIG_NGINX_X_ARCHIVE_FILES_BASE=\"{$GALAXY_CONFIG_NGINX_X_ARCHIVE_FILES_BASE:-$NGINX_GALAXY_LOCATION/_x_accel_redirect}\" - export GALAXY_CONFIG_CONDA_AUTO_INIT=\"${GALAXY_CONFIG_CONDA_AUTO_INIT:-True}\" # galaxy role variables #persistent data galaxy_persistent_directory: /export # default value galaxy_manage_mutable_setup: yes galaxy_mutable_config_dir: \"{{ galaxy_config_dir }}\" galaxy_config_style: \"yaml\" galaxy_config_file: \"{{ galaxy_config_dir }}/galaxy.yml\" #other vars galaxy_manage_database: yes # does not actually fetch eggs if galaxy uses pip galaxy_fetch_eggs: yes galaxy_vcs: git galaxy_force_checkout: yes # galaxykickstart adopts the yml standard for config file `galaxy.yml` galaxy_config: \"uwsgi\": # other uwsgi option are defaulted in ansible-galaxy-os role module: galaxy.webapps.galaxy.buildapp:uwsgi_app() # warning the above line should be dropped when prefixing # upon guide-lines in https://docs.galaxyproject.org/en/latest/admin/nginx.html logfile-chmod: 644 'galaxy': filter-with: proxy-prefix prefix: \"{{ nginx_galaxy_location }}\" # this is not clear from the galaxy.yml.sample which in addition is not consistent with https://docs.galaxyproject.org/en/latest/admin/nginx.html admin_users: \"{{ galaxy_admin }}\" database_connection: \"{{ galaxy_db }}\" tool_dependency_dir: \"{{ tool_dependency_dir }}\" ftp_upload_dir: \"{{ proftpd_files_dir }}\" ftp_upload_site: \"ftp://{{ ipify_public_ip }}\" allow_user_dataset_purge: True allow_user_impersonation: True enable_quotas: True allow_user_deletion: True allow_library_path_paste: True tool_sheds_config_file: \"{{ galaxy_config_dir }}/tool_sheds_conf.xml\" static_enabled: False watch_tool_data_dir: True use_pbkdf2: \"{{ use_pbkdf2 }}\" len_file_path: \"{{ galaxy_config_dir }}/len\" nginx_x_accel_redirect_base: /_x_accel_redirect interactive_environment_plugins_directory: config/plugins/interactive_environments visualization_plugins_directory: config/plugins/visualizations # interactive environments - set dynamic_proxy_manage to true # dynamic_proxy_manage: false dynamic_proxy_session_map: database/session_map.sqlite dynamic_proxy_bind_port: 8800 dynamic_proxy_bind_ip: 0.0.0.0 dynamic_proxy_debug: true dynamic_proxy_external_proxy: true dynamic_proxy_prefix: gie_proxy # galaxy-extras role variables galaxy_uwsgi_static_conf: true galaxy_web_processes: 2 galaxy_handler_processes: 4 supervisor_slurm_config_dir: \"{{ galaxy_data }}\" galaxy_root: \"{{ galaxy_server_dir }}\" galaxy_log_dir: \"{{ galaxy_data }}\" galaxy_database_connection: \"{{ galaxy_db }}\" galaxy_errordocs_dest: \"/usr/share/nginx/html\" galaxy_extras_config_scripts: true galaxy_extras_install_packages: true galaxy_extras_config_nginx: true galaxy_extras_config_supervisor: true galaxy_extras_config_proftpd: true galaxy_extras_config_uwsgi: false galaxy_extras_config_galaxy_job_metrics: false galaxy_extras_config_slurm: true galaxy_extras_config_galaxy_root: true supervisor_manage_nginx: true supervisor_manage_proftp: true supervisor_manage_slurm: true supervisor_manage_reports: true supervisor_manage_docker: false proftpd_nat_masquerade: false supervisor_proftpd_autostart: true galaxy_extras_config_cvmfs: false # To enable interactive environments set the following 3 variables to true galaxy_extras_config_ie_proxy: false supervisor_manage_ie_proxy: false supervisor_ie_proxy_autostart: false # specific ie for ipython and rstudio galaxy_extras_ie_fetch_jupyter: false galaxy_extras_ie_jupyter_image: quay.io/bgruening/docker-jupyter-notebook:17.09 galaxy_extras_ie_fetch_rstudio: false galaxy_extras_ie_rstudio_image: artbio/rstudio-notebook:latest # galaxy-tools role variables galaxy_tools_install_tools: true galaxy_tools_admin_user: \"{{ galaxy_admin }}\" galaxy_tools_admin_username: admin galaxy_tools_admin_user_password: \"{{ galaxy_admin_pw }}\" galaxy_tools_admin_user_preset_api_key: true default_admin_api_key: artbio2020 galaxy_tools_api_key: \"{{ default_admin_api_key }}\" galaxy_tools_create_bootstrap_user: true galaxy_tools_delete_bootstrap_user: false galaxy_tools_galaxy_instance_url: http://\"{{ galaxy_hostname }}{{ nginx_galaxy_location }}\"/ galaxy_tools_tool_list_files: [] galaxy_tools_install_workflows: true galaxy_tools_workflows: []","title":"Available variables"},{"location":"customizations/","text":"Customising the playbook \u00b6 We strongly encourage users to read the ansible inventory documentation first. Most settings should be editable without modifying the playbook directly, instead variables can be set in group_vars and host vars. The playbook comes with an example inventory file example_hosts . [artimed] localhost ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\" [travis_bioblend] localhost ansible_connection=local [aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. [artimed] , [travis_bioblend] and [aws] are predefined groups. Any host (here we only have localhost) that is added to one or multiple groups will have the corresponding group variables applied. Group variables are defined in group_vars/[name of the group] and default variables are found in group_vars/all . All variables defined in group_vars/all are overwritten in group_vars/[name of the group] . For instance the variable proftpd_nat_masquerade is set to false in group_vars/all , while hosts in the [aws] group apply the [aws] group variables which set proftpd_nat_masquerade to true, so that hosts in the aws group will have this aws-specific setting applied. Any combination of groups may be used. If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that the host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains the group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost, your inventory file should look like this: [metavisitor] localhost You can then run the playbook as usual: ansible-playbook --inventory-file=<your_inventory_file> galaxy.yml Important variables \u00b6 We aimed for this playbook to be reusable. We therefore made most variables configurable. The group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file or you can use ansible group variables to selectively set the variables for certain hosts/groups. See the ansible documentation about group variables for details. These most important variables are: ansible_ssh_user : The login name used to access the target. ansible_ssh_private_key_file : The ssh private key used to access the target. install_galaxy : True for install a Galaxy server instance. galaxy_user_name : The Operating System user name for galaxy process. galaxy_server_dir : The home of Operating System user for galaxy process. galaxy_admin : The admin galaxy user. galaxy_admin_pw : The admin galaxy password. default_admin_api_key : The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production. galaxy_tool_list : The files that constants the list of tools to be installed. galaxy_data : The persistent directory where the galaxy config and database directories will be installed or will be recovered. galaxy_database : The persistent directory where postgresql will be installed or will be recovered. galaxy_db : Connection string for galaxy-postgresql. galaxy_changeset_id : The release of Galaxy to be installed (master, dev or release_YY_MM).","title":"Customising the playbook"},{"location":"customizations/#customising-the-playbook","text":"We strongly encourage users to read the ansible inventory documentation first. Most settings should be editable without modifying the playbook directly, instead variables can be set in group_vars and host vars. The playbook comes with an example inventory file example_hosts . [artimed] localhost ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\" [travis_bioblend] localhost ansible_connection=local [aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. [artimed] , [travis_bioblend] and [aws] are predefined groups. Any host (here we only have localhost) that is added to one or multiple groups will have the corresponding group variables applied. Group variables are defined in group_vars/[name of the group] and default variables are found in group_vars/all . All variables defined in group_vars/all are overwritten in group_vars/[name of the group] . For instance the variable proftpd_nat_masquerade is set to false in group_vars/all , while hosts in the [aws] group apply the [aws] group variables which set proftpd_nat_masquerade to true, so that hosts in the aws group will have this aws-specific setting applied. Any combination of groups may be used. If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that the host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains the group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost, your inventory file should look like this: [metavisitor] localhost You can then run the playbook as usual: ansible-playbook --inventory-file=<your_inventory_file> galaxy.yml","title":"Customising the playbook"},{"location":"customizations/#important-variables","text":"We aimed for this playbook to be reusable. We therefore made most variables configurable. The group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file or you can use ansible group variables to selectively set the variables for certain hosts/groups. See the ansible documentation about group variables for details. These most important variables are: ansible_ssh_user : The login name used to access the target. ansible_ssh_private_key_file : The ssh private key used to access the target. install_galaxy : True for install a Galaxy server instance. galaxy_user_name : The Operating System user name for galaxy process. galaxy_server_dir : The home of Operating System user for galaxy process. galaxy_admin : The admin galaxy user. galaxy_admin_pw : The admin galaxy password. default_admin_api_key : The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production. galaxy_tool_list : The files that constants the list of tools to be installed. galaxy_data : The persistent directory where the galaxy config and database directories will be installed or will be recovered. galaxy_database : The persistent directory where postgresql will be installed or will be recovered. galaxy_db : Connection string for galaxy-postgresql. galaxy_changeset_id : The release of Galaxy to be installed (master, dev or release_YY_MM).","title":"Important variables"},{"location":"docker/","text":"Building and deploying galaxy-kickstart in docker \u00b6 Requirements \u00b6 You need to have docker installed and configured for your user. The repository comes with a Dockerfile that can be used to build a Docker image. This image is already available at hub.docker.com and can be pulled using the command docker pull artbio/galaxykickstart:18.04 . Building a GalaxyKickstart Docker image \u00b6 If you wish to adapt GalaxyKickStart to build a Docker image adapted to your needs, you will have to customise the Dockerfile ( Dockerfile.galaxykickstart-base ) by updating the inventory file inventory_files/docker and its dependencies group_vars/docker and extra-files/docker/ . Then, run the following command: docker build -t mygalaxykickstart . Running the available docker image from the dockerhub \u00b6 Pull the docker image: docker pull artbio/galaxykickstart:18.04 Start the image and serve it on port 8080 of your local machine in the standard docker way: CID=`docker run -d -p 8080:80 artbio/galaxykickstart:18.04` -p 8080:80 will forward requests to nginx inside the container running on port 80. Runtime changes to pre-built docker images \u00b6 If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call and galaxy will be served at http://127.0.0.1:8080/my-subdirectory . We recommend changing the default admin user as well, so the command becomes: CID=`docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" \\ -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr \\ -p 8080:80 artbio/galaxykickstart:18.04` Commit changed containers to new images \u00b6 As with standard docker containers, you can change, tag and commit running containers when you have configured them to your likings. Commit the changes to my-new-image: docker commit $CID my-new-image Stop and remove the original container: docker stop $CID && docker rm $CID Start the new container: CID=`docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" \\ -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr \\ -p 8080:80 my-new-image` Persisting to disk \u00b6 All changes made to the container are by default ephemeral. If you remove the container, the changes are gone. To persist data, including the postgresql database, galaxy's config files and your user galaxy data, mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Assuming you would like to mount your local /data folder, run CID=`docker run -d --privileged \\ -v /data:/export \\ -p 8080:80 artbio/galaxykickstart:18.04` This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data). From the new location the files are being bind-mounted back into their original location.","title":"Docker GalaxyKickStart"},{"location":"docker/#building-and-deploying-galaxy-kickstart-in-docker","text":"","title":"Building and deploying galaxy-kickstart in docker"},{"location":"docker/#requirements","text":"You need to have docker installed and configured for your user. The repository comes with a Dockerfile that can be used to build a Docker image. This image is already available at hub.docker.com and can be pulled using the command docker pull artbio/galaxykickstart:18.04 .","title":"Requirements"},{"location":"docker/#building-a-galaxykickstart-docker-image","text":"If you wish to adapt GalaxyKickStart to build a Docker image adapted to your needs, you will have to customise the Dockerfile ( Dockerfile.galaxykickstart-base ) by updating the inventory file inventory_files/docker and its dependencies group_vars/docker and extra-files/docker/ . Then, run the following command: docker build -t mygalaxykickstart .","title":"Building a GalaxyKickstart Docker image"},{"location":"docker/#running-the-available-docker-image-from-the-dockerhub","text":"Pull the docker image: docker pull artbio/galaxykickstart:18.04 Start the image and serve it on port 8080 of your local machine in the standard docker way: CID=`docker run -d -p 8080:80 artbio/galaxykickstart:18.04` -p 8080:80 will forward requests to nginx inside the container running on port 80.","title":"Running the available docker image from the dockerhub"},{"location":"docker/#runtime-changes-to-pre-built-docker-images","text":"If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call and galaxy will be served at http://127.0.0.1:8080/my-subdirectory . We recommend changing the default admin user as well, so the command becomes: CID=`docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" \\ -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr \\ -p 8080:80 artbio/galaxykickstart:18.04`","title":"Runtime changes to pre-built docker images"},{"location":"docker/#commit-changed-containers-to-new-images","text":"As with standard docker containers, you can change, tag and commit running containers when you have configured them to your likings. Commit the changes to my-new-image: docker commit $CID my-new-image Stop and remove the original container: docker stop $CID && docker rm $CID Start the new container: CID=`docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" \\ -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr \\ -p 8080:80 my-new-image`","title":"Commit changed containers to new images"},{"location":"docker/#persisting-to-disk","text":"All changes made to the container are by default ephemeral. If you remove the container, the changes are gone. To persist data, including the postgresql database, galaxy's config files and your user galaxy data, mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Assuming you would like to mount your local /data folder, run CID=`docker run -d --privileged \\ -v /data:/export \\ -p 8080:80 artbio/galaxykickstart:18.04` This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data). From the new location the files are being bind-mounted back into their original location.","title":"Persisting to disk"},{"location":"faq/","text":"Why does the playbook fail? \u00b6 Most generally, the playbook fails because of environment issues in local or remote host. Make sure that you use ansible version >= 2.9.6 in a python3 environment, that your ansible targets are under ubuntu 16.04, 18.04 or 20.04 (support to ubuntu 14.04 is dropped). Ensure also that the ssh connection ansible is relying on is appropriately specified in you inventory file (whether your run ansible locally or remotely), and that you have the root rights. You can check your ansible version by typing: ansible --version What is the username and password of the galaxy admin account ? \u00b6 Username and password of the galaxy account are controlled by the variables galaxy_admin and galaxy_admin_pw and default to admin@galaxy.org and artbio2020 (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on. If you have a host in the mygroup group, you can edit group_vars/my_group and set galaxy_admin: new_admin@email.com galaxy_admin_pw: new_password As with each change, run the playbook again. How can I set up GalaxyKickStart behind a proxy? \u00b6 Many commandline utilities can be configured to use a proxy by setting the http_proxy and https_proxy environment variables. Tasks launched by ansible will only see these environment variables if ansible sets these variables for the task. We have included a global proxy_env variable in the galaxy.yml playbook. You can set the content of this variable in your inventory or group variables (See Customizing the playbook for details on how to define variable). To use the proxy at http://proxy.bos.example.com:8080 define the variable proxy_env like so: proxy_env: http_proxy: http://proxy.bos.example.com:8080 https_proxy: http://proxy.bos.example.com:8080 no_proxy: localhost,127.0.0.0,127.0.1.1,127.0.1.1,local.home Adresses that should not be contacted through a proxy should be listed in the no_proxy variable. An example can be found in group_vars/proxy.","title":"Frequently asked questions"},{"location":"faq/#why-does-the-playbook-fail","text":"Most generally, the playbook fails because of environment issues in local or remote host. Make sure that you use ansible version >= 2.9.6 in a python3 environment, that your ansible targets are under ubuntu 16.04, 18.04 or 20.04 (support to ubuntu 14.04 is dropped). Ensure also that the ssh connection ansible is relying on is appropriately specified in you inventory file (whether your run ansible locally or remotely), and that you have the root rights. You can check your ansible version by typing: ansible --version","title":"Why does the playbook fail?"},{"location":"faq/#what-is-the-username-and-password-of-the-galaxy-admin-account","text":"Username and password of the galaxy account are controlled by the variables galaxy_admin and galaxy_admin_pw and default to admin@galaxy.org and artbio2020 (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on. If you have a host in the mygroup group, you can edit group_vars/my_group and set galaxy_admin: new_admin@email.com galaxy_admin_pw: new_password As with each change, run the playbook again.","title":"What is the username and password of the galaxy admin account ?"},{"location":"faq/#how-can-i-set-up-galaxykickstart-behind-a-proxy","text":"Many commandline utilities can be configured to use a proxy by setting the http_proxy and https_proxy environment variables. Tasks launched by ansible will only see these environment variables if ansible sets these variables for the task. We have included a global proxy_env variable in the galaxy.yml playbook. You can set the content of this variable in your inventory or group variables (See Customizing the playbook for details on how to define variable). To use the proxy at http://proxy.bos.example.com:8080 define the variable proxy_env like so: proxy_env: http_proxy: http://proxy.bos.example.com:8080 https_proxy: http://proxy.bos.example.com:8080 no_proxy: localhost,127.0.0.0,127.0.1.1,127.0.1.1,local.home Adresses that should not be contacted through a proxy should be listed in the no_proxy variable. An example can be found in group_vars/proxy.","title":"How can I set up GalaxyKickStart behind a proxy?"},{"location":"getting_started/","text":"Getting Started \u00b6 1. Requirements on the target machine (where ansible will deploy GalaxyKickStart) \u00b6 Ubuntu 16.04, 18.04 or 20.04 . Note Other systems (eg, debian) may work but they are not tested for the GalaxyKickStart development. Python >= 3.6 Note If this requirement is not satisfied, Ansible will try to install Python 3 on the target machine 2. Requirements on the Ansible machine \u00b6 Ansible Whether used remotely or locally, the Ansible version must be >= 2.9.6 Note Ansible uses ssh to send its commands. Thus, Ansible can be installed remotely (ie, on a machine that will not contain the Galaxy server at the end of the deployment), or locally (ie on the machine that will contain the Galaxy server, also called the target machine in this tutorial). In the latest case, ssh is used locally on the localhost 127.0.0.1 to chanel the commands sent by Ansible. Ansible may be installed using pip pip install ansible==2.9.2 or apt sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible git Note To clone the GalaxyKickStart GitHub repository 3. Getting the playbook \u00b6 GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/artbio/galaxyKickstart.git cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles The playbooks scripts galaxy.yml and galaxy_tool_install.yml are in the galaxykickstart folder. CONTRIBUTORS.md dockerfiles group_vars slurm_slave_node.yml Dockerfile docs inventory_files startup.sh Dockerfile.galaxykickstart-base extra-files mkdocs.yml templates LICENSE.txt galaxy.yml requirements_roles.yml README.md galaxyToSlurmCluster.yml roles ansible.cfg galaxy_tool_install.yml scripts 3. Deploying galaxy-kickstart on remote machines. \u00b6 Inside the inventory_files folder, you will find a number of inventory files. This is an example of inventory taken from the artimed inventory file. [artimed] localhost ansible_connection=local # change to the line below if remote target host # <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host, in addition to, or overwriting, the variables defined in group_vars which apply to any host. In this example, Ansible is acting locally on the localhost target. If instead you want to use Ansible remotely, replace localhost ansible_connection=local by <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" Ansible will connect by ssh to the target remote host IP , using the ssh key in path/to/your/private/key . The user specified by ansible_ssh_user=<user> may be an other user than root but needs in any case to to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/<your_inventory_file> galaxy.yml Typically, you can test using: ansible-playbook -i inventory_files/galaxy-kickstart galaxy.yml You can put multiple machines in your inventory file. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ) in group_vars/ or in group_vars/all, you will need to run the playbook again. 4. Deploying galaxy-kickstart on specified clouds \u00b6 Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine (GCE), Amazon Web Services(aws), and Jetstream (OpenStack). Please note that the ansible_user variable in the file changes for each remote target . If you are wanting to use this playbook on a cloud other than the ones listed below, you will need to update the inventory to add a new section header for the respective target. If this happens to be a cloud setup, make sure to add the section header under [cloud_setup:children] . Specifications for each remote target: GCE Image needed to deploy galaxykickstart: Ubuntu 18.04 LTS > Ubuntu 20.04 LTS > Ubuntu 16.04 LTS Inventory: <remote host IP> anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" AWS Image needed to deploy galaxykickstart: Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami-013f17f36f8b1fefb (64 bits x86) / ami-02ed82f3a38303e6f (64 bits Arm) Inventory: <target Amazon Web Services IP address> ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"<path/to/your/aws/private/key>\" Jetstream (OpenStack) Image needed to deploy galaxykickstart on Jetstream : Ubuntu 18.04 LTS Development + GUI support + Docker (jetstream image id: 15ff25f6-6ac5-4c12-b6ce-c08615ba32be) Inventory: <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" 5. Deploying galaxy-kickstart behind a proxy \u00b6 See How can I set up GalaxyKickStart behind a proxy?","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#1-requirements-on-the-target-machine-where-ansible-will-deploy-galaxykickstart","text":"Ubuntu 16.04, 18.04 or 20.04 . Note Other systems (eg, debian) may work but they are not tested for the GalaxyKickStart development. Python >= 3.6 Note If this requirement is not satisfied, Ansible will try to install Python 3 on the target machine","title":"1. Requirements on the target machine (where ansible will deploy GalaxyKickStart)"},{"location":"getting_started/#2-requirements-on-the-ansible-machine","text":"Ansible Whether used remotely or locally, the Ansible version must be >= 2.9.6 Note Ansible uses ssh to send its commands. Thus, Ansible can be installed remotely (ie, on a machine that will not contain the Galaxy server at the end of the deployment), or locally (ie on the machine that will contain the Galaxy server, also called the target machine in this tutorial). In the latest case, ssh is used locally on the localhost 127.0.0.1 to chanel the commands sent by Ansible. Ansible may be installed using pip pip install ansible==2.9.2 or apt sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible git Note To clone the GalaxyKickStart GitHub repository","title":"2. Requirements on the Ansible machine"},{"location":"getting_started/#3-getting-the-playbook","text":"GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/artbio/galaxyKickstart.git cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles The playbooks scripts galaxy.yml and galaxy_tool_install.yml are in the galaxykickstart folder. CONTRIBUTORS.md dockerfiles group_vars slurm_slave_node.yml Dockerfile docs inventory_files startup.sh Dockerfile.galaxykickstart-base extra-files mkdocs.yml templates LICENSE.txt galaxy.yml requirements_roles.yml README.md galaxyToSlurmCluster.yml roles ansible.cfg galaxy_tool_install.yml scripts","title":"3. Getting the playbook"},{"location":"getting_started/#3-deploying-galaxy-kickstart-on-remote-machines","text":"Inside the inventory_files folder, you will find a number of inventory files. This is an example of inventory taken from the artimed inventory file. [artimed] localhost ansible_connection=local # change to the line below if remote target host # <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host, in addition to, or overwriting, the variables defined in group_vars which apply to any host. In this example, Ansible is acting locally on the localhost target. If instead you want to use Ansible remotely, replace localhost ansible_connection=local by <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" Ansible will connect by ssh to the target remote host IP , using the ssh key in path/to/your/private/key . The user specified by ansible_ssh_user=<user> may be an other user than root but needs in any case to to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/<your_inventory_file> galaxy.yml Typically, you can test using: ansible-playbook -i inventory_files/galaxy-kickstart galaxy.yml You can put multiple machines in your inventory file. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ) in group_vars/ or in group_vars/all, you will need to run the playbook again.","title":"3. Deploying galaxy-kickstart on remote machines."},{"location":"getting_started/#4-deploying-galaxy-kickstart-on-specified-clouds","text":"Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine (GCE), Amazon Web Services(aws), and Jetstream (OpenStack). Please note that the ansible_user variable in the file changes for each remote target . If you are wanting to use this playbook on a cloud other than the ones listed below, you will need to update the inventory to add a new section header for the respective target. If this happens to be a cloud setup, make sure to add the section header under [cloud_setup:children] . Specifications for each remote target: GCE Image needed to deploy galaxykickstart: Ubuntu 18.04 LTS > Ubuntu 20.04 LTS > Ubuntu 16.04 LTS Inventory: <remote host IP> anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\" AWS Image needed to deploy galaxykickstart: Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami-013f17f36f8b1fefb (64 bits x86) / ami-02ed82f3a38303e6f (64 bits Arm) Inventory: <target Amazon Web Services IP address> ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"<path/to/your/aws/private/key>\" Jetstream (OpenStack) Image needed to deploy galaxykickstart on Jetstream : Ubuntu 18.04 LTS Development + GUI support + Docker (jetstream image id: 15ff25f6-6ac5-4c12-b6ce-c08615ba32be) Inventory: <remote host IP> ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"<path/to/your/private/key>\"","title":"4. Deploying galaxy-kickstart on specified clouds"},{"location":"getting_started/#5-deploying-galaxy-kickstart-behind-a-proxy","text":"See How can I set up GalaxyKickStart behind a proxy?","title":"5. Deploying galaxy-kickstart behind a proxy"},{"location":"installing_tools_and_workflows/","text":"Installing tools and workflows \u00b6 This playbook includes the ansible-galaxy-tools role which can be used to install tools and workflows into galaxy instances using the bioblend API. Importantly, in the latest GalaxyKickStart version ( v20.05 ), tool installation is performed using a separate run of ansible-playbook , typically: # these steps have should have already been performed # ansible-galaxy install -r requirements_roles.yml -p roles/ # ansible-playbook -i inventory_files/galaxy-kickstart galaxy.yml ansible-playbook -i inventory_files/galaxy-kickstart galaxy_tool_install.yml Note that this is during the galaxy_tool_install.yml ansible play that a Galaxy admin user account is created with the credentials admin@galaxy.org :artbio2020 Creating a tool_list.yml file \u00b6 Before running the galaxy_tool_install.yml playbook script as shown above, you need to prepare a tool_list.yml file with a list of tools in yaml format and with the following example content: tools: - name: blast_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: blastx_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: bowtie2 owner: devteam revisions: - 019c2a81547a tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ If the revisions key is empty, the latest available in the Galaxy toolshed revision will be installed. The tool_panel_section_label key sets the tool panel section where the tools will show up in the Galaxy tool bar. Another example of a a tool list can be found in here Obtaining a tool_list.yml file from a running Galaxy server \u00b6 You can also retrieve the list of tools running in a specific Galaxy instance using the ephemeris script workflow-to-tools . First Install ephememeris using pip pip install ephemeris This will bring several scripts in your pip environment: run-data-managers --help shed-tools install --help shed-tools update --help workflow-install --help setup-data-libraries --help get-tool-list --help workflow-to-tools --help Use the ephemeris get-tool-list to retrieve a yml list of tools installed in the Galaxy server using the command: get-tool-list -g https://usegalaxy.org -u <main galaxy username> -p <user password> --get_data_managers -o main_tools_list.yml Obtaining a tool_list.yml file from a workflow.ga galaxy file. \u00b6 You can also retrieve a list of tools used in one or more workflow galaxy files (.ga extension). These .ga files can be obtain from Galaxy server instances (menu \"download workflow file\") or other repositories Use the ephemeris workflow-to-tools to retrieve a yml list of tools using the command: workflow-to-tools -w <Galaxy-Workflow-File.ga> -l <menu_label> -o <tool_list.yml> Adding a tool_list.yml file to a group_variable files \u00b6 Group variable files are in the group_vars directory. If you would like to install tools, you need to reference the tool_list.yml in the group variable file. We typically place additional files in the extra-files/<hostname>/<hostname>_tool_list.yml file. For instance, if you would like to add tools to a group that is called metavisitor, edit group_vars/metavisitor and add these lines: install_tools: true galaxy_tools_tool_list: \"extra-files/metavisitor/metavisitor_tool_list.yml\" Installing workflows \u00b6 You can also add workflows in the Galaxy server modified by playbook script galaxy_tool_install.yml. As with tools, place the workflows in extra-files/<hostname>/<hostname><workflow_name>.ga and add these lines to the corresponding group_var file: galaxy_tools_install_workflows: true galaxy_tools_workflows: - \"extra-files/metavisitor/my_workflow_1.ga\" - \"extra-files/metavisitor/my_workflow_2.ga\" - \"extra-files/metavisitor/my_workflow_3.ga\" Running the playbook \u00b6 As mentioned earlier, after these preparation steps, you can run the playbook script galaxy_tool_install.yml with an inventory file that maps your target machine to the metavisitor group (in this example). If the target is localhost, your inventory file should look like this: [metavisitor] localhost then run the playbook like so: ansible-playbook -i inventory_files/galaxy-kickstart galaxy_tool_install.yml","title":"Installing tools and workflows"},{"location":"installing_tools_and_workflows/#installing-tools-and-workflows","text":"This playbook includes the ansible-galaxy-tools role which can be used to install tools and workflows into galaxy instances using the bioblend API. Importantly, in the latest GalaxyKickStart version ( v20.05 ), tool installation is performed using a separate run of ansible-playbook , typically: # these steps have should have already been performed # ansible-galaxy install -r requirements_roles.yml -p roles/ # ansible-playbook -i inventory_files/galaxy-kickstart galaxy.yml ansible-playbook -i inventory_files/galaxy-kickstart galaxy_tool_install.yml Note that this is during the galaxy_tool_install.yml ansible play that a Galaxy admin user account is created with the credentials admin@galaxy.org :artbio2020","title":"Installing tools and workflows"},{"location":"installing_tools_and_workflows/#creating-a-tool_listyml-file","text":"Before running the galaxy_tool_install.yml playbook script as shown above, you need to prepare a tool_list.yml file with a list of tools in yaml format and with the following example content: tools: - name: blast_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: blastx_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: bowtie2 owner: devteam revisions: - 019c2a81547a tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ If the revisions key is empty, the latest available in the Galaxy toolshed revision will be installed. The tool_panel_section_label key sets the tool panel section where the tools will show up in the Galaxy tool bar. Another example of a a tool list can be found in here","title":"Creating a tool_list.yml file"},{"location":"installing_tools_and_workflows/#obtaining-a-tool_listyml-file-from-a-running-galaxy-server","text":"You can also retrieve the list of tools running in a specific Galaxy instance using the ephemeris script workflow-to-tools . First Install ephememeris using pip pip install ephemeris This will bring several scripts in your pip environment: run-data-managers --help shed-tools install --help shed-tools update --help workflow-install --help setup-data-libraries --help get-tool-list --help workflow-to-tools --help Use the ephemeris get-tool-list to retrieve a yml list of tools installed in the Galaxy server using the command: get-tool-list -g https://usegalaxy.org -u <main galaxy username> -p <user password> --get_data_managers -o main_tools_list.yml","title":"Obtaining a tool_list.yml file from a running Galaxy server"},{"location":"installing_tools_and_workflows/#obtaining-a-tool_listyml-file-from-a-workflowga-galaxy-file","text":"You can also retrieve a list of tools used in one or more workflow galaxy files (.ga extension). These .ga files can be obtain from Galaxy server instances (menu \"download workflow file\") or other repositories Use the ephemeris workflow-to-tools to retrieve a yml list of tools using the command: workflow-to-tools -w <Galaxy-Workflow-File.ga> -l <menu_label> -o <tool_list.yml>","title":"Obtaining a tool_list.yml file from a workflow.ga galaxy file."},{"location":"installing_tools_and_workflows/#adding-a-tool_listyml-file-to-a-group_variable-files","text":"Group variable files are in the group_vars directory. If you would like to install tools, you need to reference the tool_list.yml in the group variable file. We typically place additional files in the extra-files/<hostname>/<hostname>_tool_list.yml file. For instance, if you would like to add tools to a group that is called metavisitor, edit group_vars/metavisitor and add these lines: install_tools: true galaxy_tools_tool_list: \"extra-files/metavisitor/metavisitor_tool_list.yml\"","title":"Adding a tool_list.yml file to a group_variable files"},{"location":"installing_tools_and_workflows/#installing-workflows","text":"You can also add workflows in the Galaxy server modified by playbook script galaxy_tool_install.yml. As with tools, place the workflows in extra-files/<hostname>/<hostname><workflow_name>.ga and add these lines to the corresponding group_var file: galaxy_tools_install_workflows: true galaxy_tools_workflows: - \"extra-files/metavisitor/my_workflow_1.ga\" - \"extra-files/metavisitor/my_workflow_2.ga\" - \"extra-files/metavisitor/my_workflow_3.ga\"","title":"Installing workflows"},{"location":"installing_tools_and_workflows/#running-the-playbook","text":"As mentioned earlier, after these preparation steps, you can run the playbook script galaxy_tool_install.yml with an inventory file that maps your target machine to the metavisitor group (in this example). If the target is localhost, your inventory file should look like this: [metavisitor] localhost then run the playbook like so: ansible-playbook -i inventory_files/galaxy-kickstart galaxy_tool_install.yml","title":"Running the playbook"},{"location":"metavisitor/","text":"Metavisitor is a set of Galaxy tools and workflows to detect and reconstruct viral genomes from complex deep sequence datasets. Documentation on Metavisitor installation using GalaxyKickStart is available in the Metavisitor manual","title":"Metavisitor"}]}